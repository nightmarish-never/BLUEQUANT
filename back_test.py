import torch
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from model import LSTMModel


def preprocess_data(test_data, feature_columns, target_col='close', scaler=None):
    test_data.dropna(subset=feature_columns + [target_col], inplace=True)
    valid_cols = [col for col in feature_columns if test_data[col].nunique() > 1]
    feature_columns = valid_cols

    if scaler is None:
        scaler = MinMaxScaler()
        test_data[feature_columns + [target_col]] = scaler.fit_transform(test_data[feature_columns + [target_col]])
    else:
        test_data[feature_columns + [target_col]] = scaler.transform(test_data[feature_columns + [target_col]])

    return test_data, scaler, feature_columns


def create_test_sequences(data, feature_columns, window_size, predict_days=5):
    features = data[feature_columns].values
    X = []
    for i in range(len(features) - window_size - predict_days + 1):
        X.append(features[i:i + window_size])  # ç”¨è¿‡å»window_sizeå¤©çš„æ•°æ®
    X_tensor = torch.tensor(np.array(X), dtype=torch.float32)
    return X_tensor


def backtest(model, test_tensor, device):
    model.eval()
    test_tensor = test_tensor.to(device)

    with torch.no_grad():
        predictions = model(test_tensor).cpu().numpy()  # (N, 5)
    return predictions


def inverse_scale_close(predicted, scaler, feature_columns):
    """
    predicted: shape (N, predict_days)ï¼Œä¸ºå½’ä¸€åŒ–åçš„closeé¢„æµ‹
    è¿”å›åå½’ä¸€åŒ–åçš„ close å€¼: åŒ shape
    """
    predicted = np.atleast_2d(predicted)
    num_features = len(feature_columns)
    dummy = np.zeros((predicted.shape[0], num_features + 1))  # åŒ…å« close å…± n+1 åˆ—

    results = []
    for i in range(predicted.shape[1]):  # æ¯ä¸€åˆ—é¢„æµ‹ï¼ˆå¯¹åº”æœªæ¥ç¬¬ i å¤©ï¼‰
        dummy_copy = dummy.copy()
        dummy_copy[:, -1] = predicted[:, i]  # æŠŠ close æ”¾æœ€åä¸€åˆ—
        inversed = scaler.inverse_transform(dummy_copy)
        results.append(inversed[:, -1])  # æå–åå½’ä¸€åŒ–åçš„ close å€¼

    return np.stack(results, axis=1)  # shape: (N, predict_days)


def evaluate_predictions(actual, predicted):
    for i in range(predicted.shape[1]):
        y_true = actual[:, i]
        y_pred = predicted[:, i]
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        print(f"ğŸ“‰ ç¬¬{i+1}å¤©é¢„æµ‹ MSE: {mse:.6f}, RMSE: {rmse:.6f}")


def main():
    window_size = 20
    predict_days = 5
    target_col = 'close'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åŠ è½½æ¨¡å‹
    model = LSTMModel(input_size=7, hidden_size=64, num_layers=2, output_size=predict_days).to(device)
    model.load_state_dict(torch.load('./saved_models/model_final.pth', map_location=device))

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = pd.read_csv("./training_data/test_data.csv")
    feature_columns = ["open", "high", "low", "volume", "amount", "turn", "pctChg"]

    # æ•°æ®å½’ä¸€åŒ–
    test_data, scaler, feature_columns = preprocess_data(test_data, feature_columns, target_col)

    # åˆ›å»ºæµ‹è¯•åºåˆ—
    test_tensor = create_test_sequences(test_data, feature_columns, window_size, predict_days)
    print(f"ğŸ§ª æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_tensor)}")

    # æ¨¡å‹é¢„æµ‹ï¼ˆå½’ä¸€åŒ–å€¼ï¼‰
    predicted_norm = backtest(model, test_tensor, device)

    # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ
    predicted = inverse_scale_close(predicted_norm, scaler, feature_columns)

    # æ„é€ å®é™…æœªæ¥5å¤©çš„closeä½œä¸ºè¯„ä¼°ç›®æ ‡
    actual = []
    close_values = test_data[target_col].values
    for i in range(len(test_tensor)):
        actual.append(close_values[i + window_size : i + window_size + predict_days])
    actual = np.array(actual)

    # åå½’ä¸€åŒ–å®é™… close å€¼
    actual = inverse_scale_close(actual, scaler, feature_columns)

    # è¯„ä¼°
    evaluate_predictions(actual, predicted)


if __name__ == "__main__":
    main()
